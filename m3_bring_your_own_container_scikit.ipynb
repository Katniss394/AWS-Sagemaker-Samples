{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Packaging your Algorithm \n",
    "\n",
    "Amazon SageMaker allows you to package your own algorithms and train and host it on Sagemaker. Here we package a scikit-learn implementation of decision trees for use with SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of the container\n",
    "* **Dockerfile** describes how to build your Docker container image. \n",
    "* **build_and_push.sh** script uses the Dockerfile to build your container images and then pushes it to ECR.\n",
    "* **decision_trees** contains the files that will be installed in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The files in the container are:\n",
    "\n",
    "* **nginx.conf**,the configuration file for the nginx front-end.\n",
    "* **predictor.py**,program implements the Flask web server and the decision tree predictions for this app.\n",
    "* **serve** program starts when the container is started for hosting, launches the gunicorn server which runs multiple instances of the Flask app defined in predictor.py. \n",
    "* **train**, program that is invoked when the container is run for training.\n",
    "* SageMaker will look to run an executable program named \"train\" for training and \"serve\" for hosting.\n",
    "* Or you can specify any ENTRYPOINT in your Dockerfile which has train() and serve() functions defined within.\n",
    "* **wsgi.py**,a small wrapper used to invoke the Flask app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and registering the container\n",
    "* Build the container image using docker build \n",
    "* Push the container image to ECR using docker push. \n",
    "* Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "* Looks for an ECR repository in the current account and current default region. If the repository doesn't exist, the script will create it.\n",
    "* Get the login command from ECR and execute it directly\n",
    "* Build the docker image locally with the image name \n",
    "* Push it to ECR with the full name.\n",
    "* On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order to detect your network configuration correctly.(This is a known issue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "algorithm_name=decision-trees-sample\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x decision_trees/train\n",
    "chmod +x decision_trees/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training and Hosting the Algorithm\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages \n",
    "* os -  provides a portable way of using operating system dependent functionality.\n",
    "* gmtime - Convert a time expressed in seconds since the epoch to a struct_time in UTC.\n",
    "* strftime - Convert a tuple or struct_time representing a time as returned by gmtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing some standard python packages \n",
    "* csv- module provides objects to read and write sequences.\n",
    "* itertools - module implements a number of iterator building blocks.\n",
    "* numpy  - package for scientific computing with Python.\n",
    "* pandas - library providing data structures and data analysis tools for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing amazon packages\n",
    "* boto3 - The AWS SDK for Python to write software that uses Amazon services like S3 and EC2.\n",
    "* psycopg2 - popular PostgreSQL database adapter for the Python\n",
    "* sagemaker - Python SDK for training and deploying machine learning models on Amazon SageMaker.\n",
    "* get_execution_role - Return the role ARN whose credentials are used to call the API.\n",
    "* csv_serializer - Defines csv as the behavior for serialization of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg2\n",
    "import sagemaker as sage\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=psycopg2.connect(dbname= 'loonydb1', host='myloony-db.cwwbfulhovv2.us-east-2.redshift.amazonaws.com', \n",
    "port= '5439', user= 'masteruser', password= 'Password123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"select * from public.irisdata ;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('iris.csv','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= csv.writer(fp, lineterminator='\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results:\n",
    "    print (row)\n",
    "    c.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "* Set the bucket path\n",
    "* Create a sagemaker session\n",
    "* Create a bucket and upload the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'scikit-byoc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sess.upload_data('iris.csv', key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting the model \n",
    "* Get the account and region information\n",
    "* Get the conatiner image\n",
    "* Get the IAM role credentials\n",
    "* Instantiate an estimator\n",
    "* Invoke the fit method to train the model\n",
    "* Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sess.boto_session.region_name\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '{}.dkr.ecr.{}.amazonaws.com/decision-trees-sample'.format(account, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c4.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model\n",
    "* extract some of the data we used for training\n",
    "* pass in the data to the predictor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape=pd.read_csv(\"iris.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shape[50:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[0].values.T.tolist()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X =df.drop(df.columns[0], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(test_X.values).decode('utf-8')\n",
    "results=results.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.array(names) == np.array(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
